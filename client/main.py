import os
import hashlib
import json
import requests
from urllib.parse import quote
import time
import concurrent.futures
import shutil
import zipfile
import sys

# --- å®¢æˆ¶ç«¯è¨­å®šæª” ---
config_file = "client_config.txt"
folders = {}

server_url = "http://localhost:8000"

# å¾ä¼ºæœå™¨ç²å–è³‡æ–™å¤¾åç¨±åˆ—è¡¨
try:
    print(f"æ­£åœ¨é€£ç·šä¼ºæœå™¨ä»¥ç²å–è¨­å®šæª”: {server_url}/config_names?json=1")
    resp = requests.get(f"{server_url}/config_names?json=1", timeout=10)
    if resp.status_code == 200:
        folder_names = resp.json()
    else:
        print(f"ä¼ºæœå™¨å›å‚³éŒ¯èª¤ä»£ç¢¼: {resp.status_code}")
        sys.exit(1)
except Exception as e:
    print(f"âŒ ç„¡æ³•é€£ç·šä¼ºæœå™¨ï¼Œè«‹ç¢ºèªä¼ºæœå™¨æ˜¯å¦å•Ÿå‹•ã€‚\néŒ¯èª¤: {e}")
    sys.exit(1)

# ç”Ÿæˆæœ€æ–°è¨­å®šæª”
with open(config_file, "w", encoding="utf-8") as f:
    for name in folder_names:
        f.write(f'{name}:""\n')
print(f"âœ… {config_file} å·²å»ºç«‹")

# è®€å–è¨­å®šæª”ï¼Œè§£æ force=true
for line in open(config_file, "r", encoding="utf-8"):
    line = line.strip()
    if not line or ":" not in line:
        continue
    key, path = line.split(":", 1)
    key = key.strip()
    path = path.strip().strip('"')
    force = False
    if "force=true" in key:
        key = key.replace(" force=true", "")
        force = True
    if not path:
        path = os.path.join(os.getcwd(), key)
    if not os.path.exists(path):
        os.makedirs(path)
    folders[key] = {"path": path, "force": force}


# --- MD5 ---
def get_md5(file_path):
    hash_md5 = hashlib.md5()
    try:
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except:
        return None


# --- éæ­¸æƒæ ---
def scan_folder(folder_path):
    files_md5 = {}
    for root, dirs, files in os.walk(folder_path):
        for file in files:
            full_path = os.path.join(root, file)
            rel_path = os.path.relpath(full_path, folder_path).replace("\\", "/")
            files_md5[rel_path] = get_md5(full_path)
    return files_md5


# --- çµ±è¨ˆæª”æ¡ˆæ•¸é‡ ---
def count_server_files(server_dict):
    total = 0
    for v in server_dict.values():
        if isinstance(v, dict):
            total += count_server_files(v)
        else:
            total += 1
    return total


# --- ä¸‹è¼‰ ZIP ä¸¦è§£å£“ ---
def download_and_extract_zip(zip_url, extract_to):
    zip_local = os.path.join(os.getcwd(), "temp.zip")
    try:
        print(f"ğŸ“¦ ä¸‹è¼‰ ZIP: {zip_url}")
        r = requests.get(zip_url, stream=True, timeout=30)
        with open(zip_local, "wb") as f:
            for chunk in r.iter_content(65536):
                if chunk:
                    f.write(chunk)
        print("ğŸ§© ä¸‹è¼‰å®Œæˆï¼Œé–‹å§‹è§£å£“ç¸® ...")
        with zipfile.ZipFile(zip_local, 'r') as zip_ref:
            zip_ref.extractall(extract_to)
        print("âœ… è§£å£“å®Œæˆã€‚")
    except Exception as e:
        print(f"âŒ ä¸‹è¼‰æˆ–è§£å£“å¤±æ•—: {e}")
    finally:
        if os.path.exists(zip_local):
            os.remove(zip_local)


# --- å–®æª”ä¸‹è¼‰ ---
def download_file(base_url, folder_key, file_path, save_dir, max_retries=3):
    file_url = f"{base_url}/{folder_key}/{quote(file_path)}?download=1"
    local_path = os.path.join(save_dir, file_path.replace("/", os.sep))
    os.makedirs(os.path.dirname(local_path), exist_ok=True)
    for attempt in range(max_retries):
        try:
            headers = {}
            existing_size = 0
            if os.path.exists(local_path):
                existing_size = os.path.getsize(local_path)
                headers['Range'] = f'bytes={existing_size}-'
            with requests.get(file_url, stream=True, headers=headers, timeout=15) as r:
                if r.status_code in (200, 206):
                    mode = 'ab' if existing_size > 0 else 'wb'
                    with open(local_path, mode) as f:
                        for chunk in r.iter_content(65536):
                            if chunk:
                                f.write(chunk)
                    print(f"[ä¸‹è¼‰å®Œæˆ] {folder_key}/{file_path}")
                    return True
                else:
                    print(f"[ä¸‹è¼‰å¤±æ•—] {folder_key}/{file_path}: HTTP {r.status_code}")
        except Exception as e:
            print(f"[ä¸‹è¼‰éŒ¯èª¤] {folder_key}/{file_path}: {e}")
        time.sleep(1)
    return False


# --- æ”¶é›†ä¸‹è¼‰ä»»å‹™ ---
def collect_download_tasks(server_dict, local_base, rel_path=""):
    tasks = []
    for name, value in server_dict.items():
        local_rel = f"{rel_path}/{name}" if rel_path else name
        local_abs = os.path.join(local_base, local_rel.replace("/", os.sep))
        if isinstance(value, dict):
            os.makedirs(local_abs, exist_ok=True)
            tasks.extend(collect_download_tasks(value, local_base, local_rel))
        else:
            local_md5 = get_md5(local_abs) if os.path.exists(local_abs) else None
            if not local_md5 or local_md5 != value:
                if local_md5:
                    print(f"[æ ¡é©—ç¢¼ä¸åŒ] {local_rel}")
                    os.remove(local_abs)
                else:
                    print(f"[æª”æ¡ˆç¼ºå¤±] {local_rel}")
                tasks.append(local_rel)
            else:
                print(f"[æ­£ç¢º] {local_rel}")
    return tasks


# --- ä¸»ç¨‹å¼ ---
if __name__ == "__main__":
    max_workers = 8

    for key, info in folders.items():
        folder_path = info["path"]
        force = info["force"]
        print(f"\nğŸ” æª¢æŸ¥è³‡æ–™å¤¾ {key} ...")

        if force:
            if os.path.exists(folder_path):
                print(f"[force] åˆªé™¤ {folder_path}")
                shutil.rmtree(folder_path)
            os.makedirs(folder_path)
            download_and_extract_zip(f"{server_url}/{key}?download=1", folder_path)
            continue

        try:
            resp = requests.get(f"{server_url}/{key}/?json=1", timeout=10)
            if resp.status_code != 200:
                print(f"ç„¡æ³•å–å¾—æœå‹™å™¨æª”æ¡ˆåˆ—è¡¨: {key}")
                continue
            server_files = resp.json()
        except Exception as e:
            print(f"å–å¾—æª”æ¡ˆåˆ—è¡¨å¤±æ•—: {e}")
            continue

        tasks = collect_download_tasks(server_files, folder_path)
        total_files = count_server_files(server_files)

        if total_files == 0:
            print(f"{key}: ä¼ºæœå™¨è³‡æ–™å¤¾ç‚ºç©ºï¼Œç•¥éã€‚")
            continue

        ratio = len(tasks) / total_files
        print(f"{key}: ç¼ºå¤±/ä¸åŒæª”æ¡ˆæ¯”ä¾‹ {ratio:.0%}")

        if ratio > 0.5:
            print(f"[è¶…éä¸€åŠæª”æ¡ˆéœ€è¦æ›´æ–°] ä¸‹è¼‰æ•´å€‹è³‡æ–™å¤¾ ZIP")
            shutil.rmtree(folder_path)
            os.makedirs(folder_path)
            download_and_extract_zip(f"{server_url}/{key}?download=1", folder_path)
        elif tasks:
            print(f"é–‹å§‹ä¸‹è¼‰ {len(tasks)} å€‹æª”æ¡ˆ ...")
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = [executor.submit(download_file, server_url, key, t, folder_path) for t in tasks]
                concurrent.futures.wait(futures)
        else:
            print("æ‰€æœ‰æª”æ¡ˆå‡æ­£ç¢ºï¼Œç„¡éœ€ä¸‹è¼‰ã€‚")
